{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package names to C:\\Users\\MSI\n[nltk_data]     GP63\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package names is already up-to-date!\n[nltk_data] Downloading package stopwords to C:\\Users\\MSI\n[nltk_data]     GP63\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     C:\\Users\\MSI GP63\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package vader_lexicon to C:\\Users\\MSI\n[nltk_data]     GP63\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n[nltk_data] Downloading package punkt to C:\\Users\\MSI\n[nltk_data]     GP63\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import nltk\n",
    "import csv\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from random import shuffle\n",
    "from statistics import mean\n",
    "from sklearn.naive_bayes import ( BernoulliNB,ComplementNB,MultinomialNB)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "nltk.download([\"names\",\"stopwords\",\"averaged_perceptron_tagger\",\"vader_lexicon\",\"punkt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-b92e27ee2102>, line 17)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-10-b92e27ee2102>\"\u001b[1;36m, line \u001b[1;32m17\u001b[0m\n\u001b[1;33m    print(f'La palabra 'life' aparece en total {word_count}')\u001b[0m\n\u001b[1;37m                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "#Data understanding\n",
    "with open('datasets.csv',encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    word_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            print(f'Column names are {\", \".join(row)}')\n",
    "        else:\n",
    "            tweets.append(row)\n",
    "        \n",
    "        if 'life' in row[1]:\n",
    "            word_count +=1\n",
    "        line_count += 1\n",
    "    print(f'Processed {line_count} lines.')\n",
    "    print(f'La palabra life aparece en total {word_count}')\n",
    "    print(f'First 5 tweets:\\n {tweets[:5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "#Remove stop words, RT and Non letter words\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "sentencesList=[]\n",
    "cleanedTweets = []\n",
    "for tweet in tweets:\n",
    "    text = tweet[1].replace(\"RT\",\"\")\n",
    "    cleanedTweets.append(text)\n",
    "    tweet=nltk.word_tokenize(text)\n",
    "    tweet = [w for w in tweet if w.isalpha()]\n",
    "    sentencesList.append([w for w in tweet if w.lower() not in stopwords])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.8725730278117894\nPrecision: 0.8519893111638955\nrecall: 0.925794482981126\nMeasures are:\ntp:5739\ntn:4238\nfp:997\nfn:460\npos:6199\nneg:5235\n\n"
     ]
    }
   ],
   "source": [
    "def is_positive(tweet: str) -> bool:\n",
    "    \"\"\"True if tweet has positive compound sentiment, False otherwise.\"\"\"\n",
    "    return sia.polarity_scores(tweet)[\"compound\"] > 0\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "counter = 0\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "pos = 0\n",
    "neg = 0\n",
    "for tweet in cleanedTweets:\n",
    "    val = is_positive(tweet)\n",
    "    tweetDepression = tweets[counter]\n",
    "    tweetDepression = tweetDepression[2]\n",
    "    if tweetDepression == \"1\":\n",
    "        pos = pos +1\n",
    "        if val:\n",
    "            tp  = tp +1\n",
    "        else:\n",
    "            fn = fn + 1\n",
    "    else:\n",
    "        neg = neg +1\n",
    "        if val:\n",
    "            fp = fp +1\n",
    "        else:\n",
    "            tn = tn + 1\n",
    "    counter = counter + 1\n",
    "accuracy = (tp+tn)/(pos+neg)\n",
    "precision = (tp/(tp+fp))\n",
    "recall = (tp/(tp+fn))\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'recall: {recall}')\n",
    "print(f'Measures are:\\ntp:{tp}\\ntn:{tn}\\nfp:{fp}\\nfn:{fn}\\npos:{pos}\\nneg:{neg}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}